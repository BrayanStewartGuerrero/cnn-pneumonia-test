{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas de directorios de datos\n",
    "train_dir = './DATA/TRAIN'\n",
    "val_dir = './DATA/VAL'\n",
    "test_dir = './DATA/TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones de las imágenes\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4020 images belonging to 3 classes.\n",
      "Found 420 images belonging to 3 classes.\n",
      "Found 39 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Configurar el preprocesamiento de imágenes y generadores de datos\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo VGG16 pre-entrenado sin la capa densa superior\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar capas densas superiores personalizadas\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir el modelo base y las capas densas personalizadas\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelar los pesos del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar modelo\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir callbacks\n",
    "early_stop = EarlyStopping(patience=5, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7825 - accuracy: 0.6439 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Redes Neuronales\\pneumonia_artificial_neural_network\\env\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1617s 13s/step - loss: 0.7825 - accuracy: 0.6439 - val_loss: 0.6469 - val_accuracy: 0.8173\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1630s 13s/step - loss: 0.5822 - accuracy: 0.7197 - val_loss: 0.6672 - val_accuracy: 0.8173\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1631s 13s/step - loss: 0.5279 - accuracy: 0.7588 - val_loss: 0.5545 - val_accuracy: 0.8365\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1629s 13s/step - loss: 0.5040 - accuracy: 0.7731 - val_loss: 0.7094 - val_accuracy: 0.8077\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1631s 13s/step - loss: 0.4811 - accuracy: 0.7693 - val_loss: 0.6107 - val_accuracy: 0.8365\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1630s 13s/step - loss: 0.4828 - accuracy: 0.7844 - val_loss: 0.7069 - val_accuracy: 0.8269\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1624s 13s/step - loss: 0.4735 - accuracy: 0.7683 - val_loss: 0.8338 - val_accuracy: 0.8005\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1620s 13s/step - loss: 0.4781 - accuracy: 0.7633 - val_loss: 0.7346 - val_accuracy: 0.7812\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "        callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la curva de pérdida\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Curva de pérdida')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.show()\n",
    "\n",
    "# Graficar la curva de precisión\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.title('Curva de precisión')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar resultados de entrenamiento y evaluación\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo con conjunto de prueba\n",
    "model.load_weights('best_model.h5')\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from metrics import confusion_matrix\n",
    "\n",
    "# Obtener las etiquetas predichas para los datos de prueba\n",
    "y_pred = modelo.predict(test_generator)\n",
    "\n",
    "# Convertir las etiquetas predichas en clases (0 o 1) mediante umbral de 0.5\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Obtener las verdaderas clases para los datos de prueba\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Obtener la lista de etiquetas (nombres de las clases) a partir del generador\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print('Matriz de confusión:')\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import classification_report\n",
    "\n",
    "# Cargar los pesos del modelo\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Obtener las predicciones del modelo en el conjunto de prueba\n",
    "predictions = model.predict_generator(test_generator)\n",
    "\n",
    "# Obtener las etiquetas verdaderas del conjunto de prueba\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Obtener el reporte de clasificación para cada clase\n",
    "report = classification_report(true_labels, np.argmax(predictions, axis=1), target_names=test_generator.class_indices.keys(), output_dict=True)\n",
    "\n",
    "# Crear una figura de barras para la precisión, recuperación y puntuación F1\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ind = np.arange(len(report.keys())-3)\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(['precision', 'recall', 'f1-score']):\n",
    "    scores = [report[label][metric] for label in test_generator.class_indices.keys()]\n",
    "    ax.bar(ind+i*width, scores, width, label=metric)\n",
    "\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(test_generator.class_indices.keys())\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel('Clases')\n",
    "ax.set_ylabel('Valor')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para predecir una imagen\n",
    "def classify_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_width, img_height))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0\n",
    "\n",
    "    prediction = model.predict(x)[0]\n",
    "\n",
    "    classes = ['Normal', 'Neumonia_Bacteria', 'Neumonia_Virus']\n",
    "    for i in range(len(classes)):\n",
    "        print(f'{classes[i]}: {prediction[i] * 100:.2f}%')\n",
    "\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    return classes[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_width, img_height))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0\n",
    "\n",
    "    prediction = model.predict(x)[0]\n",
    "\n",
    "    classes = ['Normal', 'Neumonia_Bacteria', 'Neumonia_Virus']\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    # Crear dos subplots: uno para la imagen y otro para las probabilidades\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    # Mostrar la imagen en el primer subplot\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Imagen')\n",
    "\n",
    "    # Crear una lista de colores para las barras de las probabilidades\n",
    "    colors = ['g' if i == predicted_class else 'r' for i in range(len(classes))]\n",
    "\n",
    "    # Mostrar las probabilidades en un gráfico de barras horizontal en el segundo subplot\n",
    "    ax2.barh(classes, prediction * 100, color=colors)\n",
    "    ax2.set_xlim([0, 100])\n",
    "    ax2.set_title('Probabilidades')\n",
    "    ax2.set_xlabel('%')\n",
    "\n",
    "    # Ajustar los subplots y mostrar la figura\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return classes[predicted_class]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
